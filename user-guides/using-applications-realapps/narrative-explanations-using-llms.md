---
description: >-
  LLMs make it possible to convert your explanations into readable,
  natural-language sentences.
---

# Narrative explanations (using LLMs)

{% hint style="info" %}
The full code for this and all other user guides can be found in our [user guide tutorial](https://github.com/sibyl-dev/pyreal/blob/dev/tutorials/user\_guide.ipynb).
{% endhint %}

You may prefer to get explanations in natural-language sentences. For example, instead of a table or graph of feature contributions, you may prefer what we call a _narrative explanation_:

> The predicted price of the house increased significantly (by about $20,000) because of its location in the expensive neighborhood of Sawyer. However, the house is smaller than average (at 605 sq. ft.) and does not have AC, which reduced its predicted price by about $12,000.

To generate narrative explanations accurately, we plug the explanations generated by Pyreal into a _large language model_ (LLM), which automatically converts it into sentence format.

Currently, Pyreal requires an [OpenAI API key](https://openai.com/pricing) to use its narrative explanation functionality, which will incur a cost. We hope to add free options for this functionality soon!

To generate narrative explanations, you can either pass your API key into your RealApp at initialization, or to the produce function.&#x20;

At the moment, the only supported narrative explanation function is `.produce_narrative_feature_contributions`, which generates a feature contributions explanation in sentence format.

```python
realapp.set_openai_client(openai_api_key="YOUR_OPENAI_KEY")
narratives = realapp.produce_narrative_feature_contributions(x_input)

narratives["House 101"]
# Sample output: 
# The model predicts a lower price for a house with average quality 
# finishing and materials and a smaller above ground living area...
```

## Tuning narratives

There are several key input parameters to the `produce_narrative_feature_contributions` function when tuning your narrative explanations:

* <mark style="color:purple;">num\_features</mark> (_int,_ default=5): the number of features to include in the narrative.&#x20;
* <mark style="color:purple;">llm\_model</mark> (_one of "gpt3.5", "gpt4",_ default="gpt3.5"): the LLM model to use. GPT4 may produce better narratives, but will be more expensive.
* <mark style="color:purple;">detail\_level</mark> (_one of "high", "low",_ default="high): high detail narratives will explicitly list all contribution values, while low detail will tend to discuss only the general contribution direction. You can tune this further with training, see the [Improving Narratives](narrative-explanations-using-llms.md#improving-narratives) section on this page.
* <mark style="color:purple;">context\_description</mark> (_string_): A brief sentence explaining what the model does. For example, "The model predicts the price of houses". This can also be passed into the RealApp object at initialization instead.
* <mark style="color:purple;">max\_tokens</mark> (_int,_ default=200): The maximum [number of tokens](https://platform.openai.com/tokenizer) to include in the explanation narratives. Larger values allow for longer narratives, but may cost more. Larger num\_feature values require more tokens (roughly 40 tokens per feature seems to work well).

## Improving Narratives

You can train the LLM on what kinds of explanations you want. This is called _few-shot learning_. The LLM asks you for some examples of explanations, and then learns from those examples.

To run this training, you can use the `.train_feature_contribution_llm`. This function will show a few feature contributions explanations, and ask you for narrative versions using Python input/output.

For example, let's say you want very basic explanations that just list out important features:

```python
# num_inputs -> the number of explanations to show and ask for narrative versions of
#               you will probably see results with ~3-5 inputs.
# num_features -> the number of features to include per explanation.
realapp.train_feature_contribution_llm(num_inputs=2, num_features=2)
```

The function above will trigger an I/O chain:

> <mark style="color:orange;">PYREAL</mark>: For each of the following inputs, please provide an appropriate narrative version.
>
> <mark style="color:orange;">PYREAL</mark>: Input 1 (feature, value, contribution): (Lot size, 1039, 1230), (Material, Brick, 2930)
>
> <mark style="color:orange;">PYREAL</mark><mark style="color:blue;">:</mark> Narrative explanation ('q' to quit):&#x20;
>
> <mark style="color:blue;">USER</mark>: The house's predicted price increased because of its lot size and material
>
> <mark style="color:orange;">PYREAL</mark>: Input 2 (feature, value, contribution): (AC, No, -1320), (Location, Sawyer, 4306)
>
> <mark style="color:orange;">PYREAL</mark>: Narrative explanation ('q' to quit):&#x20;
>
> <mark style="color:blue;">USER</mark>: The lack of AC reduced the predicted price, while the location increased it.

Future narrative explanations generated by this RealApp will now mirror the structure of the user-given examples - in this case, by mentioning important features and their direction of contribution.
